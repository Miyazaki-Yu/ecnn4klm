{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ecnn4klm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ecnn4klm` can perform large-scale and fast calculations of localized spin dynamics and energy evaluations by substituting the computation of itinerant electrons in the Kondo lattice model. In this tutorial, we will cover data generation, model definition, and training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insatll ecnn4klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/Miyazaki-Yu/ecnn4klm.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the training and validation data, we perform exact diagonalization on small lattice sizes ($16\\times 16$) to reduce computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from ecnn4klm.util.exact import klmsq2d\n",
    "from ecnn4klm.util.spin_generator import random_spin\n",
    "from ecnn4klm.model import SpinConvSq2dSparse, NonLinear2d, SelfInt2d\n",
    "from torch import nn\n",
    "import e3nn\n",
    "from e3nn.o3 import Irreps, Irrep\n",
    "from e3nn import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 100\n",
    "N_test = 50\n",
    "L = 16\n",
    "Lx = L\n",
    "Ly = L\n",
    "t = 1.0\n",
    "J = 7.0\n",
    "filling = 0.485\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = N_train + N_test\n",
    "spin_data = random_spin(N,(Lx,Ly),dtype=torch.float32)\n",
    "E_data, B_data, _ = klmsq2d(spin_data, t=t, J=J, filling=filling)\n",
    "spin_train = spin_data[:N_train].to(torch.float32).to(device)\n",
    "E_train = E_data[:N_train].to(torch.float32).to(device)\n",
    "B_train = B_data[:N_train].to(torch.float32).to(device)\n",
    "spin_test = spin_data[N_train:].to(torch.float32).to(device)\n",
    "E_test = E_data[N_train:].to(torch.float32).to(device)\n",
    "B_test = B_data[N_train:].to(torch.float32).to(device)\n",
    "batch_size = 1\n",
    "train_set = data.TensorDataset(spin_train, E_train, B_train) \n",
    "train_loader = data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_set = data.TensorDataset(spin_test, E_test, B_test) \n",
    "test_loader = data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ecnn4klm` is a PyTorch-based library, and neural network models can be defined by inheriting from `nn.Module`. In defining the model, the notation of irreducible representations from the `e3nn` library is used. For more information, please refer to the e3nn website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l_i = 2\n",
    "        l_f = 1\n",
    "        k = 2\n",
    "        l_hid = 2\n",
    "        c = 8\n",
    "        ir_hid = Irreps([(c,(l,1)) for l in range(l_hid+1)])\n",
    "       \n",
    "        self.conv1 = SpinConvSq2dSparse(io.SphericalTensor(self.l_i, p_val=1, p_arg=1), l_f, ir_hid, kernel_size=k)\n",
    "        self.act1 = NonLinear2d(ir_hid, bias = False)\n",
    "        self.conv2 = SpinConvSq2dSparse(ir_hid, l_f, ir_hid, kernel_size=k)\n",
    "        self.act2 = NonLinear2d(ir_hid, bias = False)\n",
    "        self.conv3 = SpinConvSq2dSparse(ir_hid, l_f, ir_hid, kernel_size=k)\n",
    "        self.act3 = NonLinear2d(ir_hid, bias = False)\n",
    "        self.conv4 = SpinConvSq2dSparse(ir_hid, l_f, ir_hid, kernel_size=k)\n",
    "        self.act4 = NonLinear2d(ir_hid, bias = False)  \n",
    "\n",
    "        self.convout = SpinConvSq2dSparse(ir_hid, l_f, \"8x0e\", kernel_size=k)      \n",
    "        \n",
    "        self.selfintout1 = SelfInt2d(\"8x0e\", \"4x0e\", biases=True)\n",
    "        self.actout1 = NonLinear2d(\"4x0e\", bias = False)\n",
    "        self.selfintout2 = SelfInt2d(\"4x0e\", \"2x0e\", biases=True)\n",
    "        self.actout2 = NonLinear2d(\"2x0e\", bias = False)\n",
    "        self.selfintout3 = SelfInt2d(\"2x0e\", \"1x0e\", biases=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,spin):\n",
    "        feat = e3nn.o3.spherical_harmonics(list(range(self.l_i+1)), spin, normalize=False)\n",
    "\n",
    "        feat = self.conv1(feat,spin)\n",
    "        feat_sk = self.act1(feat)\n",
    "        feat = self.conv2(feat_sk,spin)\n",
    "        feat_sk = self.act2(feat+feat_sk)\n",
    "        feat = self.conv3(feat_sk,spin)\n",
    "        feat_sk = self.act3(feat+feat_sk)    \n",
    "        feat = self.conv4(feat_sk,spin)\n",
    "        feat_sk = self.act4(feat+feat_sk)\n",
    " \n",
    "        feat = self.convout(feat_sk,spin)\n",
    "        \n",
    "        feat = self.selfintout1(feat)\n",
    "        feat = self.actout1(feat)\n",
    "        feat = self.selfintout2(feat)\n",
    "        feat = self.actout2(feat)\n",
    "        feat = self.selfintout3(feat)\n",
    "        return feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "optimizer = torch.optim.Adagrad(net.parameters(), lr=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the neural network is performed. With this configuration, it should typically take around 30 minutes using a GPU backend. If you want to further improve the accuracy, try increasing the number of epochs or adjusting various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_losses_E = [] \n",
    "train_losses_B = []\n",
    "test_losses = []\n",
    "test_losses_E = []\n",
    "test_losses_B = []\n",
    "lamb = 0.0\n",
    "time_start = time.perf_counter()\n",
    "best_epoch = 0\n",
    "for epoch in range(30):\n",
    "    net.train()\n",
    "    train_loss = []\n",
    "    train_loss_E = [] \n",
    "    train_loss_B = []\n",
    "    for spin,E,B in train_loader: \n",
    "    \n",
    "        spin.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        E_pred = net(spin).sum(dim=(1,2,3))\n",
    "        B_pred = -torch.autograd.grad(E_pred[0], spin, create_graph=True)[0]\n",
    "        # B_pred = -torch.autograd.grad(E_pred, spin, torch.eye(batch_size,dtype=torch.float32).to(device), create_graph=True, is_grads_batched=True)[0].sum(dim=1)\n",
    "        loss_E = loss_func(E,E_pred)/ (Lx*Ly)**2 \n",
    "        loss_B = loss_func(B,B_pred) \n",
    "        loss = loss_B + loss_E * lamb\n",
    "        train_loss.append(loss.item())\n",
    "        train_loss_E.append(loss_E.item()) # with E\n",
    "        train_loss_B.append(loss_B.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(sum(train_loss)/len(train_loss))\n",
    "    train_losses_E.append(sum(train_loss_E)/len(train_loss_E)) # with E\n",
    "    train_losses_B.append(sum(train_loss_B)/len(train_loss_B))\n",
    "\n",
    "    test_loss = []\n",
    "    test_loss_E = [] # with E\n",
    "    test_loss_B = []\n",
    "    net.eval()\n",
    "    for spin,E,B in test_loader: # with E\n",
    "    \n",
    "        spin.requires_grad_(True)\n",
    "        E_pred = net(spin).sum(dim=(1,2,3))\n",
    "        B_pred = -torch.autograd.grad(E_pred[0], spin, create_graph=True)[0]\n",
    "        # B_pred = -torch.autograd.grad(E_pred, spin, torch.eye(batch_size,dtype=torch.float32).to(device), create_graph=True, is_grads_batched=True)[0].sum(dim=1)\n",
    "        loss_E = loss_func(E,E_pred)/ (Lx*Ly)**2 # with E\n",
    "        loss_B = loss_func(B,B_pred)\n",
    "        loss = loss_B + loss_E * lamb\n",
    "        test_loss.append(loss.item())\n",
    "        test_loss_E.append(loss_E.item()) # with E\n",
    "        test_loss_B.append(loss_B.item())\n",
    "        \n",
    "    test_losses.append(sum(test_loss)/len(test_loss))\n",
    "    test_losses_E.append(sum(test_loss_E)/len(test_loss_E)) # with E\n",
    "    test_losses_B.append(sum(test_loss_B)/len(test_loss_B))\n",
    "    time_now = time.perf_counter()\n",
    "\n",
    "    print(f\"epoch {epoch}: lr:{optimizer.param_groups[0]['lr']:.3e}, train:{train_losses[epoch]}, test:{test_losses[epoch]}, elapsed: {time_now-time_start} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
